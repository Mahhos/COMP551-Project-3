import numpy as np
import matplotlib.pyplot as plt
import random
import pandas as pd
import time
from sklearn import linear_model, datasets
from keras.layers import Conv2D, Reshape, Flatten, Dropout, BatchNormalization
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.optimizers import Adam

original_train_x = np.load("C:/Users/Dell/Documents/HEC/Session 2/Mcgill Machine Learning/Dev 3/tinyX.npy")
original_train_y = np.load("C:/Users/Dell/Documents/HEC/Session 2/Mcgill Machine Learning/Dev 3/tinyY.npy")

kaggle = np.load("C:/Users/Dell/Documents/HEC/Session 2/Mcgill Machine Learning/Dev 3/tinyX_test.npy")

idx = random.sample(range(np.size(original_train_y)), np.size(original_train_y))

totX = original_train_x[idx]
totY = original_train_y[idx]


totX = np.reshape(totX, (26344,12288))
kaggle = np.reshape(kaggle, (6600,12288))

#testX = totX
#testY = totY

#trainX, test = totX[:round((0.8*len(totX)),0),:], totX[round((0.8*len(totX)),0):,:]

                    
temp = pd.DataFrame(totX)
temp['Y'] = totY

tempX, testX = np.split(temp.sample(frac=1), [int(.8*len(temp))])

#i = 0
#tempX = temp.loc[temp.Y == i].head(140)
#i +=1
#while i < 40:
#    temp2 = temp.loc[temp.Y == i].head(140)
#    tempX = tempX.append(temp2)
#    i+=1
trainY = tempX.Y.values
del tempX['Y']
trainX = tempX.values

testY = testX.Y.values
del testX['Y']
testX = testX.values

logreg = linear_model.LogisticRegression()


logreg.fit(trainX, trainY)

Z = logreg.predict(testX)

df = pd.DataFrame(Z)
df['class'] = testY

df['correct'] = np.where(df[0] == df['class'],1,0)

good_class_test = df.correct.mean()*100

submission = pd.DataFrame(logreg.predict(kaggle))

submission.columns = ['class']

submission.to_csv("C:/Users/Dell/Documents/HEC/Session 2/Mcgill Machine Learning/Dev 3/LogReg.csv")

model = Sequential()

model.add(Reshape((3,64,64), input_shape=(12288,)))
model.add(Conv2D(32, 3, 3, border_mode='same', subsample=(2,2), activation='relu'))
model.add(Conv2D(32, 3, 3, border_mode='same', subsample=(2,2), activation='relu'))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(output_dim=64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(output_dim=40, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

#model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0005), metrics=['accuracy'])

model.fit(trainX, trainY, nb_epoch=20)

# test it!
validation_metrics = model.evaluate(testX, testY)
print()
print(validation_metrics)

Z = model.predict(kaggle)

Z = pd.DataFrame(Z)
Z = pd.DataFrame(Z.idxmax(axis=1))
Z.columns = ['class']
Z.to_csv("C:/Users/Dell/Documents/HEC/Session 2/Mcgill Machine Learning/Dev 3/keras20e2cetadam.csv", index=True, index_label="id")
